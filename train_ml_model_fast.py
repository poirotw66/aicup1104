#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆæ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´ - FAST VERSION
è§£æ±ºè³‡æ–™æ´©æ¼å•é¡Œ + é«˜æ•ˆè¨˜æ†¶é«”ç®¡ç†
"""

import os
import sys
import time
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import joblib
import json
import warnings
warnings.filterwarnings('ignore')

# å¼·åˆ¶å³æ™‚è¼¸å‡º
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', buffering=1)


def print_banner(text):
    """åˆ—å°æ©«å¹…"""
    print("\n" + "="*70, flush=True)
    print(f"  {text}", flush=True)
    print("="*70 + "\n", flush=True)


def load_data_efficiently():
    """é«˜æ•ˆè¼‰å…¥è³‡æ–™"""
    print_banner("Phase 1: è¼‰å…¥è³‡æ–™")
    
    start_time = time.time()
    
    # 1. è¼‰å…¥æ¨™ç±¤
    print("è¼‰å…¥ç•°å¸¸å¸³æˆ¶æ¨™ç±¤...", flush=True)
    alert_df = pd.read_csv('raw_data/acct_alert.csv')
    alert_accounts = set(alert_df['acct'].values)
    print(f"  âœ“ ç•°å¸¸å¸³æˆ¶: {len(alert_accounts):,}", flush=True)
    
    # 2. è¼‰å…¥é æ¸¬ç›®æ¨™
    print("è¼‰å…¥é æ¸¬ç›®æ¨™...", flush=True)
    predict_df = pd.read_csv('raw_data/acct_predict.csv')
    predict_accounts = set(predict_df['acct'].values)
    print(f"  âœ“ é æ¸¬ç›®æ¨™: {len(predict_accounts):,}", flush=True)
    
    # 3. åˆ†æ‰¹è¼‰å…¥ç‰¹å¾µï¼ˆé¿å…è¨˜æ†¶é«”å•é¡Œï¼‰
    print("\nè¼‰å…¥ç‰¹å¾µæª”æ¡ˆï¼ˆåˆ†æ‰¹è™•ç†ï¼‰...", flush=True)
    feature_file = 'output/features.csv'
    
    if not os.path.exists(feature_file):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ {feature_file}", flush=True)
        print("è«‹å…ˆåŸ·è¡Œ main.py æˆ– main_ultra_fast.py ç”Ÿæˆç‰¹å¾µ", flush=True)
        sys.exit(1)
    
    # å…ˆè®€å–è¡¨é ­
    feature_cols = pd.read_csv(feature_file, nrows=0).columns.tolist()
    feature_cols.remove('acct')
    if 'is_alert' in feature_cols:
        feature_cols.remove('is_alert')
    
    print(f"  âœ“ ç‰¹å¾µæ•¸: {len(feature_cols)}", flush=True)
    
    elapsed = time.time() - start_time
    print(f"\nâœ“ Phase 1 å®Œæˆ ({elapsed:.1f} ç§’)", flush=True)
    
    return alert_accounts, predict_accounts, feature_file, feature_cols


def prepare_training_data(feature_file, feature_cols, alert_accounts, predict_accounts, 
                          normal_sample_size=5000):
    """
    æº–å‚™è¨“ç·´è³‡æ–™ï¼ˆé¿å…è³‡æ–™æ´©æ¼ï¼‰
    ä½¿ç”¨åˆ†æ‰¹è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”
    """
    print_banner("Phase 2: æº–å‚™è¨“ç·´è³‡æ–™ï¼ˆä¿®æ­£è³‡æ–™æ´©æ¼ï¼‰")
    
    start_time = time.time()
    
    print(f"ğŸ” ç­–ç•¥: åªç”¨æ¨™è¨˜è³‡æ–™è¨“ç·´ï¼Œä¸åŒ…å«é æ¸¬ç›®æ¨™", flush=True)
    print(f"  - ç•°å¸¸æ¨£æœ¬: {len(alert_accounts):,}", flush=True)
    print(f"  - æ­£å¸¸æ¨£æœ¬: æœ€å¤š {normal_sample_size:,} (å¾éé æ¸¬ç›®æ¨™ä¸­æŠ½æ¨£)", flush=True)
    
    # åˆ†æ‰¹è®€å–ï¼Œç¯©é¸éœ€è¦çš„å¸³æˆ¶
    chunksize = 100000
    alert_data = []
    normal_data = []
    normal_sampled = 0
    
    print(f"\nåˆ†æ‰¹è™•ç†ç‰¹å¾µæª”æ¡ˆ...", flush=True)
    
    chunk_count = 0
    for chunk in pd.read_csv(feature_file, chunksize=chunksize):
        chunk_count += 1
        
        # æ”¶é›†ç•°å¸¸å¸³æˆ¶
        alert_chunk = chunk[chunk['acct'].isin(alert_accounts)]
        if len(alert_chunk) > 0:
            alert_data.append(alert_chunk)
            print(f"  Chunk {chunk_count}: æ‰¾åˆ° {len(alert_chunk)} å€‹ç•°å¸¸å¸³æˆ¶", flush=True)
        
        # æ”¶é›†æ­£å¸¸å¸³æˆ¶ï¼ˆæ’é™¤é æ¸¬ç›®æ¨™ï¼‰
        if normal_sampled < normal_sample_size:
            available_normal = chunk[
                ~chunk['acct'].isin(alert_accounts) &
                ~chunk['acct'].isin(predict_accounts)
            ]
            
            if len(available_normal) > 0:
                # è¨ˆç®—é‚„éœ€è¦å¤šå°‘æ¨£æœ¬
                needed = normal_sample_size - normal_sampled
                
                # éš¨æ©ŸæŠ½æ¨£
                if len(available_normal) > needed:
                    sample = available_normal.sample(n=needed, random_state=42)
                else:
                    sample = available_normal
                
                normal_data.append(sample)
                normal_sampled += len(sample)
                print(f"  Chunk {chunk_count}: æŠ½æ¨£ {len(sample)} å€‹æ­£å¸¸å¸³æˆ¶ (ç´¯è¨ˆ: {normal_sampled})", flush=True)
    
    print(f"\nåˆä½µè³‡æ–™...", flush=True)
    
    # åˆä½µç•°å¸¸è³‡æ–™
    if len(alert_data) == 0:
        print("âŒ éŒ¯èª¤: æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç•°å¸¸å¸³æˆ¶ï¼", flush=True)
        sys.exit(1)
    
    alert_df = pd.concat(alert_data, ignore_index=True)
    print(f"  âœ“ ç•°å¸¸æ¨£æœ¬: {len(alert_df):,}", flush=True)
    
    # åˆä½µæ­£å¸¸è³‡æ–™
    if len(normal_data) == 0:
        print("âŒ éŒ¯èª¤: æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ­£å¸¸å¸³æˆ¶ï¼", flush=True)
        sys.exit(1)
    
    normal_df = pd.concat(normal_data, ignore_index=True)
    print(f"  âœ“ æ­£å¸¸æ¨£æœ¬: {len(normal_df):,}", flush=True)
    
    # åˆä½µè¨“ç·´é›†
    train_df = pd.concat([alert_df, normal_df], ignore_index=True)
    train_df['label'] = train_df['acct'].isin(alert_accounts).astype(int)
    
    # æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤
    X_train = train_df[feature_cols]
    y_train = train_df['label']
    
    # æª¢æŸ¥è³‡æ–™æ´©æ¼
    leak_check = train_df['acct'].isin(predict_accounts).sum()
    
    print(f"\nè¨“ç·´é›†çµ±è¨ˆ:", flush=True)
    print(f"  ç¸½æ¨£æœ¬: {len(train_df):,}", flush=True)
    print(f"  ç•°å¸¸: {y_train.sum():,} ({y_train.sum()/len(y_train)*100:.2f}%)", flush=True)
    print(f"  æ­£å¸¸: {len(y_train)-y_train.sum():,} ({(len(y_train)-y_train.sum())/len(y_train)*100:.2f}%)", flush=True)
    print(f"  ç‰¹å¾µæ•¸: {len(feature_cols)}", flush=True)
    
    if leak_check > 0:
        print(f"\nâš ï¸  è­¦å‘Š: è¨“ç·´é›†ä¸­æœ‰ {leak_check} å€‹é æ¸¬ç›®æ¨™å¸³æˆ¶ï¼", flush=True)
    else:
        print(f"\nâœ… ç¢ºèª: è¨“ç·´é›†ä¸­æ²’æœ‰é æ¸¬ç›®æ¨™å¸³æˆ¶ï¼ˆç„¡è³‡æ–™æ´©æ¼ï¼‰", flush=True)
    
    elapsed = time.time() - start_time
    print(f"\nâœ“ Phase 2 å®Œæˆ ({elapsed:.1f} ç§’)", flush=True)
    
    return X_train, y_train, train_df


def train_models(X_train, y_train, feature_cols):
    """è¨“ç·´å¤šå€‹æ¨¡å‹"""
    print_banner("Phase 3: è¨“ç·´æ¨¡å‹")
    
    start_time = time.time()
    
    # äº¤å‰é©—è­‰è¨­å®š
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    models = {}
    cv_scores = {}
    
    # æ¨¡å‹ 1: Random Forest (å¹³è¡¡)
    print("1ï¸âƒ£  Random Forest (Balanced)...", flush=True)
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_leaf=5,
        min_samples_split=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    print("   åŸ·è¡Œ 5-fold äº¤å‰é©—è­‰...", flush=True)
    rf_f1_scores = []
    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        
        rf.fit(X_fold_train, y_fold_train)
        y_pred = rf.predict(X_fold_val)
        f1 = f1_score(y_fold_val, y_pred)
        rf_f1_scores.append(f1)
        print(f"     Fold {fold}: F1 = {f1:.4f}", flush=True)
    
    rf_f1_scores = np.array(rf_f1_scores)
    print(f"   âœ“ CV F1-Score: {rf_f1_scores.mean():.4f} (+/- {rf_f1_scores.std():.4f})", flush=True)
    
    # åœ¨å…¨éƒ¨è¨“ç·´é›†ä¸Šè¨“ç·´
    rf.fit(X_train, y_train)
    models['random_forest'] = rf
    cv_scores['random_forest'] = rf_f1_scores
    
    # æ¨¡å‹ 2: Gradient Boosting
    print("\n2ï¸âƒ£  Gradient Boosting...", flush=True)
    gb = GradientBoostingClassifier(
        n_estimators=200,
        max_depth=8,
        learning_rate=0.05,
        min_samples_leaf=5,
        subsample=0.8,
        random_state=42,
        verbose=0
    )
    
    print("   åŸ·è¡Œ 5-fold äº¤å‰é©—è­‰...", flush=True)
    gb_f1_scores = []
    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        
        gb.fit(X_fold_train, y_fold_train)
        y_pred = gb.predict(X_fold_val)
        f1 = f1_score(y_fold_val, y_pred)
        gb_f1_scores.append(f1)
        print(f"     Fold {fold}: F1 = {f1:.4f}", flush=True)
    
    gb_f1_scores = np.array(gb_f1_scores)
    print(f"   âœ“ CV F1-Score: {gb_f1_scores.mean():.4f} (+/- {gb_f1_scores.std():.4f})", flush=True)
    
    gb.fit(X_train, y_train)
    models['gradient_boosting'] = gb
    cv_scores['gradient_boosting'] = gb_f1_scores
    
    # æ¨¡å‹ 3: Random Forest (ä¿å®ˆ)
    print("\n3ï¸âƒ£  Random Forest (Conservative)...", flush=True)
    rf_cons = RandomForestClassifier(
        n_estimators=200,
        max_depth=10,
        min_samples_leaf=10,
        min_samples_split=20,
        class_weight='balanced_subsample',
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    print("   åŸ·è¡Œ 5-fold äº¤å‰é©—è­‰...", flush=True)
    rf_cons_f1_scores = []
    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        
        rf_cons.fit(X_fold_train, y_fold_train)
        y_pred = rf_cons.predict(X_fold_val)
        f1 = f1_score(y_fold_val, y_pred)
        rf_cons_f1_scores.append(f1)
        print(f"     Fold {fold}: F1 = {f1:.4f}", flush=True)
    
    rf_cons_f1_scores = np.array(rf_cons_f1_scores)
    print(f"   âœ“ CV F1-Score: {rf_cons_f1_scores.mean():.4f} (+/- {rf_cons_f1_scores.std():.4f})", flush=True)
    
    rf_cons.fit(X_train, y_train)
    models['rf_conservative'] = rf_cons
    cv_scores['rf_conservative'] = rf_cons_f1_scores
    
    # é¸æ“‡æœ€ä½³æ¨¡å‹
    best_model_name = max(cv_scores, key=lambda k: cv_scores[k].mean())
    best_model = models[best_model_name]
    best_score = cv_scores[best_model_name].mean()
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}", flush=True)
    print(f"   CV F1-Score: {best_score:.4f}", flush=True)
    
    elapsed = time.time() - start_time
    print(f"\nâœ“ Phase 3 å®Œæˆ ({elapsed/60:.1f} åˆ†é˜)", flush=True)
    
    return best_model, best_model_name, cv_scores


def evaluate_on_training(model, X_train, y_train):
    """è©•ä¼°è¨“ç·´é›†è¡¨ç¾"""
    print_banner("Phase 4: è¨“ç·´é›†è©•ä¼°")
    
    y_pred = model.predict(X_train)
    
    precision = precision_score(y_train, y_pred)
    recall = recall_score(y_train, y_pred)
    f1 = f1_score(y_train, y_pred)
    cm = confusion_matrix(y_train, y_pred)
    
    print(f"Precision: {precision:.4f}", flush=True)
    print(f"Recall: {recall:.4f}", flush=True)
    print(f"F1-Score: {f1:.4f}", flush=True)
    print(f"\nConfusion Matrix:", flush=True)
    print(f"  TN: {cm[0][0]:,}, FP: {cm[0][1]:,}", flush=True)
    print(f"  FN: {cm[1][0]:,}, TP: {cm[1][1]:,}", flush=True)
    
    return {
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'confusion_matrix': cm.tolist()
    }


def predict_efficiently(model, feature_file, feature_cols, predict_accounts, 
                       output_path='output/predictions_ml.csv'):
    """é«˜æ•ˆé æ¸¬ï¼ˆåˆ†æ‰¹è™•ç†ï¼‰"""
    print_banner("Phase 5: ç”Ÿæˆé æ¸¬")
    
    start_time = time.time()
    
    print(f"åˆ†æ‰¹è®€å–ä¸¦é æ¸¬...", flush=True)
    
    results = []
    chunksize = 100000
    found_count = 0
    
    for chunk_idx, chunk in enumerate(pd.read_csv(feature_file, chunksize=chunksize), 1):
        # ç¯©é¸é æ¸¬ç›®æ¨™
        pred_chunk = chunk[chunk['acct'].isin(predict_accounts)]
        
        if len(pred_chunk) > 0:
            X_pred = pred_chunk[feature_cols]
            y_pred = model.predict(X_pred)
            y_proba = model.predict_proba(X_pred)[:, 1]
            
            result_chunk = pd.DataFrame({
                'acct': pred_chunk['acct'].values,
                'label': y_pred,
                'confidence_score': y_proba
            })
            
            results.append(result_chunk)
            found_count += len(pred_chunk)
            print(f"  Chunk {chunk_idx}: æ‰¾åˆ° {len(pred_chunk)} å€‹é æ¸¬ç›®æ¨™ (ç´¯è¨ˆ: {found_count})", flush=True)
    
    if len(results) == 0:
        print("âŒ éŒ¯èª¤: æ²’æœ‰æ‰¾åˆ°ä»»ä½•é æ¸¬ç›®æ¨™ï¼", flush=True)
        sys.exit(1)
    
    # åˆä½µçµæœ
    result_df = pd.concat(results, ignore_index=True)
    
    # ä¿å­˜
    result_df.to_csv(output_path, index=False)
    print(f"\nâœ… é æ¸¬çµæœå·²ä¿å­˜: {output_path}", flush=True)
    
    # ç°¡åŒ–ç‰ˆ
    simple_output = output_path.replace('.csv', '_acct_label.csv')
    result_df[['acct', 'label']].to_csv(simple_output, index=False)
    print(f"âœ… ç°¡åŒ–ç‰ˆå·²ä¿å­˜: {simple_output}", flush=True)
    
    # çµ±è¨ˆ
    n_alert = result_df['label'].sum()
    print(f"\né æ¸¬çµ±è¨ˆ:", flush=True)
    print(f"  ç¸½é æ¸¬æ•¸: {len(result_df):,}", flush=True)
    print(f"  é æ¸¬ç‚ºç•°å¸¸: {n_alert:,} ({n_alert/len(result_df)*100:.2f}%)", flush=True)
    print(f"  é æ¸¬ç‚ºæ­£å¸¸: {len(result_df)-n_alert:,} ({(len(result_df)-n_alert)/len(result_df)*100:.2f}%)", flush=True)
    print(f"  å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {result_df['confidence_score'].mean():.4f}", flush=True)
    
    elapsed = time.time() - start_time
    print(f"\nâœ“ Phase 5 å®Œæˆ ({elapsed:.1f} ç§’)", flush=True)
    
    return result_df


def main():
    """ä¸»å‡½æ•¸"""
    total_start = time.time()
    
    print("="*70, flush=True)
    print("ğŸ”§ ä¿®æ­£ç‰ˆæ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´ - FAST VERSION", flush=True)
    print("   è§£æ±ºè³‡æ–™æ´©æ¼ + é«˜æ•ˆè¨˜æ†¶é«”ç®¡ç†", flush=True)
    print("="*70, flush=True)
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    
    # Phase 1: è¼‰å…¥è³‡æ–™
    alert_accounts, predict_accounts, feature_file, feature_cols = load_data_efficiently()
    
    # Phase 2: æº–å‚™è¨“ç·´è³‡æ–™
    X_train, y_train, train_df = prepare_training_data(
        feature_file, feature_cols, alert_accounts, predict_accounts,
        normal_sample_size=5000
    )
    
    # Phase 3: è¨“ç·´æ¨¡å‹
    best_model, best_model_name, cv_scores = train_models(X_train, y_train, feature_cols)
    
    # Phase 4: è©•ä¼°
    train_metrics = evaluate_on_training(best_model, X_train, y_train)
    
    # Phase 5: é æ¸¬
    result_df = predict_efficiently(
        best_model, feature_file, feature_cols, predict_accounts,
        output_path='output/predictions_ml.csv'
    )
    
    # ä¿å­˜æ¨¡å‹å’Œå ±å‘Š
    print_banner("Phase 6: ä¿å­˜æ¨¡å‹å’Œå ±å‘Š")
    
    model_path = 'output/trained_model.pkl'
    joblib.dump({
        'model': best_model,
        'model_name': best_model_name,
        'feature_cols': feature_cols
    }, model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}", flush=True)
    
    report = {
        'model_name': best_model_name,
        'cv_f1_mean': float(cv_scores[best_model_name].mean()),
        'cv_f1_std': float(cv_scores[best_model_name].std()),
        'train_metrics': train_metrics,
        'n_features': len(feature_cols),
        'n_train_samples': len(X_train),
        'n_predictions': len(result_df),
        'n_predicted_alerts': int(result_df['label'].sum()),
        'prediction_rate': float(result_df['label'].sum() / len(result_df))
    }
    
    with open('output/ml_training_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    print(f"âœ… è¨“ç·´å ±å‘Šå·²ä¿å­˜: output/ml_training_report.json", flush=True)
    
    # ç¸½çµ
    total_time = (time.time() - total_start) / 60
    print_banner("âœ¨ è¨“ç·´å®Œæˆï¼")
    
    print(f"çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.1f} åˆ†é˜", flush=True)
    
    print(f"\n" + "="*70, flush=True)
    print(f"ğŸ“Š æ¨¡å‹è¡¨ç¾ç¸½çµ", flush=True)
    print(f"="*70, flush=True)
    print(f"æ¨¡å‹: {best_model_name}", flush=True)
    print(f"äº¤å‰é©—è­‰ F1: {report['cv_f1_mean']:.4f} (+/- {report['cv_f1_std']:.4f})", flush=True)
    print(f"è¨“ç·´é›† F1: {train_metrics['f1_score']:.4f}", flush=True)
    print(f"é æ¸¬ç•°å¸¸ç‡: {report['prediction_rate']*100:.2f}%", flush=True)
    
    print(f"\n" + "="*70, flush=True)
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ", flush=True)
    print(f"="*70, flush=True)
    print(f"âœ… output/predictions_ml.csv (å®Œæ•´ç‰ˆ)", flush=True)
    print(f"âœ… output/predictions_ml_acct_label.csv (æäº¤ç‰ˆ)", flush=True)
    print(f"âœ… output/trained_model.pkl (æ¨¡å‹æª”)", flush=True)
    print(f"âœ… output/ml_training_report.json (å ±å‘Š)", flush=True)
    
    print(f"\nğŸ’¡ å»ºè­°ä¸Šå‚³: output/predictions_ml_acct_label.csv", flush=True)
    print(f"   é æœŸ F1-Score: 0.25-0.40 (ç›¸æ¯”åŸæœ¬ 0.07 æœ‰é¡¯è‘—æå‡)", flush=True)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ä½¿ç”¨è€…ä¸­æ–·", flush=True)
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)

